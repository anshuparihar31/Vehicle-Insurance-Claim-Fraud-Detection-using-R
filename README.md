# ğŸš—ğŸ” Vehicle Insurance Fraud Claim  Detection

## ğŸ“Œ Project Overview  
Vehicle insurance fraud occurs when individuals collaborate to deceive or overstate the extent of damage or injuries resulting from a car accident. This project utilizes **machine learning techniques** to detect fraudulent claims effectively.

## ğŸ“‚ Dataset  
ğŸ“Œ **Source:** [Kaggle](https://www.kaggle.com/datasets/shivamb/vehicle-claim-fraud-detection)  
ğŸ“Œ **Total Rows:** 15,420  
ğŸ“Œ **Total Columns:** 33  
ğŸ“Œ **Numeric Features:** Age, Repnumber, Deductible, DriverRating  
ğŸ“Œ **Target Variable:** `FraudFound_P` (0 = No Fraud, 1 = Fraud)  

## ğŸ›  Data Preprocessing  
âœ”ï¸ **Categorical Encoding:** Categorical variables are encoded as factors.  
âœ”ï¸ **Missing Value Handling:** No explicit missing value handling in the dataset.  
âœ”ï¸ **Oversampling:** The dataset is **oversampled using the ROSE package** to handle class imbalance.

## ğŸ“Š Feature Selection Methods  

ğŸ”¹ **1. Mutual Information (MI)**  
- Measures the amount of information one variable provides about another.  
- Identifies the most relevant features for fraud detection.  
- Top **17 features** are selected for model training.

ğŸ”¹ **2. Mean Decrease in Gini Impurity**  
- Used with **Random Forest & Decision Trees**.  
- Determines feature importance based on impurity reduction in trees.  

## ğŸ¤– Algorithm Implementation  

### ğŸŒ³ **Decision Tree (`rpart`)**  
ğŸ“Œ **Method:** Classification (`method = "class"`)  
ğŸ“Œ **Hyperparameter:** `cp = 0.001` (controls tree complexity to avoid overfitting)  

### ğŸŒ² **Random Forest (`randomForest`)**  
ğŸ“Œ **Hyperparameters:**  
âœ”ï¸ `mtry = 6` (number of variables randomly sampled for each split)  
âœ”ï¸ `ntree = 700` (number of trees in the forest)  
âœ”ï¸ `nodesize = 5` (minimum terminal node size)  

### ğŸ§® **Naive Bayes**  
ğŸ“Œ **Hyperparameters:**  
âœ”ï¸ Laplace smoothing (`.laplace`)  
âœ”ï¸ Kernel density estimates (`.usekernel`)  
âœ”ï¸ Bandwidth adjustment (`.adjust`)  

### ğŸ“ˆ **Logistic Regression (`glmnet`)**  
ğŸ“Œ **Hyperparameters:**  
âœ”ï¸ `alpha = 0:1` (LASSO `alpha = 1` and Ridge `alpha = 0`)  
âœ”ï¸ `lambda` values from **0.01 to 1** (controls regularization strength)  

### âœ¨ **Support Vector Machine (SVM)**  
ğŸ“Œ **Hyperparameter:** `C = 1` (controls the balance between error minimization and model complexity) 

## ğŸ“Š Results  

### **Classifier Accuracies Before Feature Selection:**  
| Model              | Accuracy (%) |
|--------------------|-------------|
| Random Forest (RF) | **89.6%**    |
| Decision Tree (DT) | 78.7%        |
| Support Vector Machine (SVM) | 76.52%  |
| NaÃ¯ve Bayes       | 77.59%       |
| Logistic Regression | 77.02%     |

### **Accuracies After Feature Selection (Mutual Information - 17 features):**  
| Model             | Accuracy (%) |
|-------------------|-------------|
| Random Forest    | **90.47%**    |
| Decision Tree    | 80.33%        |

### **Accuracies After Feature Selection (Mean Decrease Gini Impurity - 19 features):**  
| Model            | Accuracy (%) |
|------------------|-------------|
| Random Forest   | **90.83%**    |
| Decision Tree   | 79.98%        |
| SVM             | 76.13%        |

### **Accuracies Using 14 Common Features in the Shiny App:**  
| Model           | Accuracy (%) |
|----------------|-------------|
| Random Forest  | **87.74%**   |
| SVM           | 79.10%        |
| Decision Tree  | 74.88%        |

ğŸ“Œ **Best Model:** **Random Forest** consistently achieved the highest accuracy of **87.74%** using the **14 common features in the Shiny app**.  

ğŸ“Œ **AUC Scores:**  
- **Random Forest (Before Feature Selection):** **0.91**  
- **Random Forest (Shiny App - 14 common features):** **0.959** (+0.04 improvement)  

## ğŸ–¥ï¸ Shiny App  
A **Shiny app** was developed to predict whether an insurance claim is **Fraudulent** or **Not Fraudulent** using the **14 common features**. The app utilizes the **Random Forest model**, achieving **high accuracy and AUC value of 0.959**.  

## ğŸ”¹ How It Works?
### ğŸ“ User Inputs Claim Details  
The app provides an interactive form where users enter details such as:  
- **Vehicle Age**  
- **Policy Number**  
- **Claim Amount**  
- **Driver Rating**  
- **Policy State**  
- **Month of Claim**  
- **Days to Claim Reporting**  
- **Accident Severity**  
- **Make of the Vehicle**  
- **Vehicle Price**  
- **Age of Policyholder**  
- **Past Claims**  
- **Number of Witnesses**  
- **Police Report Filed** (Yes/No)  
 

### âš¡ Model Prediction  
- The **Random Forest model** processes the inputs and predicts whether the claim is **fraudulent (1) or not (0)**.  

### ğŸ“Š Results Visualization  
- The app displays the **fraud probability** along with key **contributing factors**.  

## ğŸ”¹ Technology Stack  

- **Frontend:** Shiny (R-based UI framework)  
- **Backend:** R (Machine Learning with `randomForest` and `glmnet`)  
- **Data Handling:** `tidyverse`, `dplyr`, `caret`  
- **Deployment:** Hosted on **ShinyApps.io**  

### ğŸ›  **Shiny App Dependencies**  
Ensure the following R packages are installed:  
```sh
install.packages(c("shiny", "randomForest", "shinythemes", "dplyr", "ggplot2", "caret", "ROSE"))
```

## ğŸš€ Installation & Setup  

### ğŸ”¹ **1. Clone the Repository**  
```sh
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
```
ï»¿
